# Alfred tries fine-tuning type shi fine shyt

# THIS FILE CREATES A FINE TUNED MODEL BASED ON CLEANED INPUT (fixed_data.jsonl)
# THE END OF THIS FILE ALSO TESTS THE FINE TUNED MODEL ON AN INPUTTED SENTENCE

# ğŸ“Œ Step 1: Import Necessary Libraries
import json
import numpy as np
import datasets
import torch
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    TrainingArguments,
    Trainer,
    pipeline
)

# ğŸ“Œ Step 2: Load Dataset
dataset = load_dataset("json", data_files={"train": "fixed_data.jsonl"}, split="train")

# ğŸ“Œ Step 3.1: Load Tokenizer 
model_name = "dslim/bert-base-NER"  # Base BERT model
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ğŸ“Œ Step 3.2: Define label-to-ID mapping
LABEL_MAP = {
    "O": 0,  # Outside any entity
    "INS": 1,  # Institution
    "CNS": 2,  # Constitution
    "STA": 3,  # Statute
    "RA": 4,  # Republic Act
    "PROM_DATE": 5,  # Promulgation Date
    "CASE_NUM": 6,  # Case Number
    "PERSON": 7  # Person
}

# Reverse mapping for model output interpretation
id2label = {v: k for k, v in LABEL_MAP.items()}  # Converts 0 -> "O", 1 -> "INS", etc.
label2id = {k: v for k, v in LABEL_MAP.items()}  # Converts "INS" -> 1, etc.

# ğŸ“Œ Step 3.3: Load Model with Custom Labels
model = AutoModelForTokenClassification.from_pretrained(
    model_name, 
    num_labels=len(LABEL_MAP), 
    id2label=id2label, 
    label2id=label2id, 
    ignore_mismatched_sizes=True
)

# ğŸ“Œ Step 4: Tokenize and Align Labels
import torch

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["text"], truncation=True, padding="max_length", max_length=512)

    labels = []
    for i, label in enumerate(examples["entities"]):  # Ensure looping through all entities
        label_ids = [-100] * len(tokenized_inputs["input_ids"][i])  # Default to -100 (ignored in loss)
        
        for entity in label:
            start, end, entity_label = entity["start"], entity["end"], entity["label"]
            for idx in range(len(tokenized_inputs["input_ids"])):
                char_pos = tokenized_inputs.char_to_token(i, idx)
                if char_pos is None:
                    continue  # Skip tokens that don't map to original characters

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Apply function
tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)

for batch in tokenized_dataset:
    print("Input length:", len(batch["input_ids"]))
    print("Labels length:", len(batch["labels"]))

# ğŸ“Œ Step 5: Define Training Arguments
training_args = TrainingArguments(
    output_dir="./bert-legal-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    num_train_epochs=5,  # Adjust based on dataset size
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=10,
    report_to="none"
)

# ğŸ“Œ Step 6: Fine-Tune the Model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,  # Change if you have a separate test dataset
    tokenizer=tokenizer
)

# Check label distribution in dataset
# from collections import Counter

# all_labels = [label for data in tokenized_dataset["labels"] for label in data if label != -100]
# label_counts = Counter(all_labels)

# print("Label Distribution:", {id2label[k]: v for k, v in label_counts.items()})


trainer.train()

# ğŸ“Œ Step 7: Evaluate Performance
metric = evaluate.load("seqeval")

def compute_metrics(eval_preds):
    predictions, labels = eval_preds

    # Convert logits to class indices
    predictions = np.argmax(predictions, axis=2)

    # Remove -100 values (ignored tokens) from labels and predictions
    true_labels = [
        [label for label, pred in zip(label_list, pred_list) if label != -100]
        for label_list, pred_list in zip(labels, predictions)
    ]

    pred_labels = [
        [pred for label, pred in zip(label_list, pred_list) if label != -100]
        for label_list, pred_list in zip(labels, predictions)
    ]

    # Check if we have valid labels before computing metrics
    if not any(true_labels):  # If all true labels are empty
        print("Error: No valid labels to compute metrics!")
        return {"f1": 0.0, "precision": 0.0, "recall": 0.0, "accuracy": 0.0}  # Return default values

    # Compute metrics
    results = metric.compute(predictions=pred_labels, references=true_labels)

    return {
        "f1": results.get("overall_f1", 0.0),
        "precision": results.get("overall_precision", 0.0),
        "recall": results.get("overall_recall", 0.0),
        "accuracy": results.get("overall_accuracy", 0.0),
    }

# Run evaluation with predictions
predictions, labels, _ = trainer.predict(tokenized_dataset)  # Get predictions instead of just eval loss

# Compute Metrics
metrics = compute_metrics((predictions, labels))

# ğŸ“Œ Step 8: Save and Deploy Model
model.save_pretrained("./bert-legal-ner")
tokenizer.save_pretrained("./bert-legal-ner")

# Upload to Hugging Face Hub (Optional)
# model.push_to_hub("your-username/philippines-legal-ner")
# tokenizer.push_to_hub("your-username/philippines-legal-ner")

# ğŸ“Œ Step 9: Test the Fine-Tuned Model with a test set
nlp = pipeline("ner", model="./bert-legal-ner", tokenizer="./bert-legal-ner", aggregation_strategy="first")

# Input test set
text = "Also, Sec. 11 of RA 3019 was similarly amended by Sec. 15, Art. XI of the 1987 Constitution with respect to imprescriptibility of offenses related to ill-gotten wealth.\nNonetheless, granting that the offense committed by respondents is not imprescriptible, the reckoning point shall not be from the execution of the MOA on November 20, 1974, but from the EDSA Revolution in February 1986. The Republic, citing Sec. 2 of Act No. 3326,which provides that \"prescription shall begin to run from the day of the commission of the violation of law, and if the same be not known at the time, from the discovery thereof and the institution of judicial proceedings,\" argues that the reckoning point of the prescriptive period must be from the discovery of the alleged violation,\u00a0i.e., at the time of the EDSA Revolution in February 1986 and not from the execution of the MOA on November 20, 1974.\nRepublic explains that the acts complained of were committed during the Marcos regime by persons closely associated with President Marcos, which means that no one could have known the existence of the said MOA dated November 20, 1974 except respondents themselves. Even assuming that third parties knew of the existence of the subject MOA, no one had the reasonable opportunity nor political will to prosecute respondents or the persons involved therein. Considering the peculiar circumstances at that time, the prescriptive period should be reckoned from the discovery of the offense,\u00a0i.e., immediately after the EDSA Revolution in February 1986."
results = nlp(text)


# Convert numeric labels to actual entity names
for entity in results:
    label = entity["entity_group"]  # Extract label
    if label.startswith("LABEL_"):  # If still in numeric format (e.g., "LABEL_3")
        entity["entity_group"] = id2label.get(int(label.replace("LABEL_", "")), "O")  # Map it
    # Otherwise, keep the existing label if it's already mapped


# Assuming 'results' contains the NER output
print("â—† NER Output:")
for entity in results:
    print(f"{entity['word']} -> {entity['entity_group']}")
print(f"ğŸ”¹ F1 Score: {metrics['f1']:.4f}")
print(f"ğŸ”¹ Precision: {metrics['precision']:.4f}")
print(f"ğŸ”¹ Recall: {metrics['recall']:.4f}")
